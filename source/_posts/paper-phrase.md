---
title: 拯救糟糕的英文写作之论文中经常使用的句式
date: 2019-11-11 10:48:55
tags: paper
categories: paper
---

日积月累，以备写作之需。

<!--more-->

### 摘要

1.We also **experimentally demonstrate the effectiveness of** ListRank-MF **by comparing its performance with that of** item-based collaborative recommendation and a related state-of-th-art collaborative filtering ranking approach (CoFiRank).



2.We **perform extensive experiments on** two large real-world CF datasets, **and the results clearly show the effectiveness and efficiency of our proposed model.**



3.**yield better efficiency and effectiveness** on two real datasets.



4.**We conduct extensive experiments on four publicly accessible benchmarks, showing signifiicant improvements relative to several state-of-the-art** collaborative filtering and graph neural network-based recommendation models. **Further experiments quantitatively verify the effectiveness of each component of our proposed model and demonstrate that** the learned embeddings capture the important relationship structure.

###引言

1.Collaborative filtering (CF) **has been regarded as one of the most successful** recommender techniques.

2.These models **show competing performance** for task such as xxx, xxx, and so on.

3.These GCN based recommender models **show better performance compared to** traditional models.

4.degrade/enhance the recommendation performance.

5.**Extensive results demonstrate the superiority of** xxx **over the strongest state-of-the-art models**.

6.The remaining of this article is organized as follows:

7.**The rest of this paper is organized as follows. We first provide some preliminaries** for ICF in Section 2. **We then elaborate our proposed** DeepICF methods in Section 3. **Afterwards we report experimental results in** 4 and review related work in Section 5. **Finally we conclude the paper and highlight some future directions** in Section 6. 

### 模型

1.有研究表明：It's reported that/As reported in xx, 

2.**Before we dive into the details** of this survey/ **Before going further into** different sections

#### 时间复杂度分析

1.**we analyze the time complexity of** NGCF.

2.For the l-th hidden layer, the multiplication between matrices and vectors **is the main operation which can be done in O()**.

3.The prediction layer **only involves** inner product of two vectors, **for which the complexity is** 

4.**As such, 􏰈􏰈the overall time complexity for** evaluating a DeepICF model is 

5.**Therefore, the overall time cost of evaluating a prediction with** DeepICF+a is 

#### 泛化

1.can be viewed as a special case of / can also be seen as a instance of

### 实验

1.Xxx **achieves a performance improvement of** ca. xx% **over** xx.

2.**Keeping the rest parameters constant, we did a full parameter study for different values of** α. 

3.**To test the different parameters to study the value of** α, **we fixed the remaining parameter values.** 

4.without special mention / unless stated differently / unless specified / unless otherwise stated / without additional explanation / without special declaration



5.As we can see, /We can find that/ It can be seen clearly that

We can make the following observations



6.To verify this/ In order to verify this



7.We evaluate ConvNCF of the specific setting **as illustrated in Table/Figure** 2.

8.**The experimental results are provied in Table/Figure** x.

9.超过第二好的方法多少，超过最好的baseline

Multi-GCCF **consistently yields the best performance for all datasets**. **More precisely,** Multi-GCCF **improves over the strongest baselines with respect to** recall@20 **by** 9.01%, 12.19%, 5.52%, and 3.10% **for** Yelp2018, Amazon-CDs, Amazon-Books and Gowalla, **respectively**. Multi-GCCF **further outperforms the strongest baselines by** 12.41%, 15.43%, 24.54% and 6.39% **on** recall@20 **for** Yelp2018, Amazon-CDs, Amazon-Books and Gowalla, respectively, when increasing the latent dimension. **For the** NDCG@20 **metric**, Multi-GCCF **outperforms the next best method by** 5% to 25% on three dataset.

10.**The above findings provide empirical evidence for the rationality and effectiveness of** optimizing the log loss for learning from implicit data. 

11.show/report/provide/depict

### 结论

1.**Extensive experiments on two real-world datasets demonstrate the superior perfor mance of our proposed model compared with the state-of-the-art methods.** 

2.符号表示

denote A by B，用B表示A

set A to B，将A设置为B

refer to A as B 将A称为B



replace A by B 	用B替换A



to be specific / specifically



to be fair



leave ... as future work



other than



take ... into consideration/account



account for



provide A with B

distinguish A from B

associate A with B

relate A to B

combine A with B

replace A with B 用B代替A



be formulated as 归结为



include but are not limited to n. / doing 包括但不限于 





be limited by



notably 显著地；尤其



rely on



in addition



as a consequence 因此

to this end



notably / noticeably 显著地



namely 



to our best knowledge / to the best of our knowledge 据我们所知



be prone to doing



It is worth pointing out that ...

prevent ... from ...



for the purpose of ...



as such / as a result  因此



take ... as an example.



enrich/enhance/augment



use/utilize/employ/adopt



combine/integrate



investigate/study



distinguish/discriminate/differentiate